{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8149a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.12.1\n",
      "Torchvision Version:  0.13.1\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from __future__ import print_function, division, absolute_import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e954155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, num_classes=1000, \n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "#         x = self.head(x)\n",
    "        return x\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9f64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "from timm.data.constants import \\\n",
    "    IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
    "from timm.data import create_transform\n",
    "\n",
    "\n",
    "def build_transform(is_train, args):\n",
    "    resize_im = args.input_size > 32\n",
    "    imagenet_default_mean_and_std = args.imagenet_default_mean_and_std\n",
    "    mean = IMAGENET_INCEPTION_MEAN if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_MEAN\n",
    "    std = IMAGENET_INCEPTION_STD if not imagenet_default_mean_and_std else IMAGENET_DEFAULT_STD\n",
    "\n",
    "    if is_train:\n",
    "        # this should always dispatch to transforms_imagenet_train\n",
    "        transform = create_transform(\n",
    "            input_size=args.input_size,\n",
    "            is_training=True,\n",
    "            color_jitter=args.color_jitter,\n",
    "            auto_augment=args.aa,\n",
    "            interpolation=args.train_interpolation,\n",
    "            re_prob=args.reprob,\n",
    "            re_mode=args.remode,\n",
    "            re_count=args.recount,\n",
    "            mean=mean,\n",
    "            std=std,\n",
    "        )\n",
    "        if not resize_im:\n",
    "            transform.transforms[0] = transforms.RandomCrop(\n",
    "                args.input_size, padding=4)\n",
    "        return transform\n",
    "\n",
    "    t = []\n",
    "    if resize_im:\n",
    "        # warping (no cropping) when evaluated at 384 or larger\n",
    "        if args.input_size >= 384:  \n",
    "            t.append(\n",
    "            transforms.Resize((args.input_size, args.input_size), \n",
    "                            interpolation=transforms.InterpolationMode.BICUBIC), \n",
    "        )\n",
    "            print(f\"Warping {args.input_size} size input images...\")\n",
    "        else:\n",
    "            if args.crop_pct is None:\n",
    "                args.crop_pct = 224 / 256\n",
    "            size = int(args.input_size / args.crop_pct)\n",
    "            t.append(\n",
    "                # to maintain same ratio w.r.t. 224 images\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BICUBIC),  \n",
    "            )\n",
    "            t.append(transforms.CenterCrop(args.input_size))\n",
    "\n",
    "    t.append(transforms.ToTensor())\n",
    "    t.append(transforms.Normalize(mean, std))\n",
    "    return transforms.Compose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871c2576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../CLIP/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4acf647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import clip\n",
    "\n",
    "class CustomDataSet(data.Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        self.root = root\n",
    "        self.samples = []\n",
    "        self.label = []\n",
    "        cnt = [0 for i in range(5)]\n",
    "        for i in range(5):\n",
    "            self.samples.extend([root + str(i) + '/' + path_image for path_image in os.listdir(root + '/' + str(i))])\n",
    "            self.label.extend([i for j in range(len(os.listdir(root + '/' +str(i))))])\n",
    "            cnt[i] += len(os.listdir(root + '/' + str(i)))\n",
    "        print(cnt)\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target = self.label[index]\n",
    "        sample = Image.open(self.samples[index]).convert('RGB')\n",
    "        if(self.transform):\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        feature = X_train[self.samples[index]]\n",
    "            \n",
    "        return sample, feature, target\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.samples)\n",
    "        return length\n",
    "    def get_labels(self):\n",
    "        return self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b904c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model, Preprocess = clip.load(\"ViT-B/32\", device=\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ab080",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"food\", \"drink\", \"dirty\", \"woman\", \"man\", \n",
    "                \"face\", \"messy\", \"untidy\", \"watermark\", \"blur\", \n",
    "                \"paper\", \"page\", \"person\", \"behavior\", \"road\", \n",
    "                \"house\", \"food\", \"drink\", \"woman\", \"man\", \n",
    "                \"body\", \"crowd\", \"vehicle\", \"poster\", \"food\", \n",
    "                \"drink\", \"sharpness\", \"natural scene\", \"organized\", \"sharpness\", \n",
    "                \"tidiness\", \"food\", \"drink\", \"natural scene\", \"garden, field\", \n",
    "                \"very organized\", \"sharpness\", \"tidiness\", \"food, drink\", \n",
    "                \"natural scene\", \"garden field\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97e8212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7977, 20927, 11643, 1418, 33]\n",
      "[2660, 6976, 3882, 473, 11]\n"
     ]
    }
   ],
   "source": [
    "data_train = CustomDataSet('../Data/data70k/train/', False)\n",
    "data_test = CustomDataSet('../Data/data70k/test/', False)\n",
    "\n",
    "# data_train = CustomDataSet1('../Data/data70k/train/', [], isTrain = True)\n",
    "# data_test = CustomDataSet1('../Data/data70k/test/', [], isTrain = False)\n",
    "\n",
    "X_train, X_test = {}, {}\n",
    "\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# text=[\"human\",\n",
    "#      \"human body parts\",\n",
    "#      \"nice picture\",\n",
    "#      \"bad image\",\n",
    "#      \"natural landscape\",\n",
    "#      \"leftovers\",\n",
    "#      \"delicious food\",\n",
    "#      \"animal\"]\n",
    "text = torch.cat([clip.tokenize(f\"{c}\") for c in text]).to(device)\n",
    "\n",
    "# image_features = Model.encode_image(inputs.unsqueeze(0).to(device))\n",
    "with torch.no_grad():\n",
    "    text_features = Model.encode_text(text)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a270194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"vector_train_naver2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a896db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 56000/56000 [00:11<00:00, 4952.15it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = {}, {}\n",
    "\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    X_train[train.loc[i][\"Unnamed: 0\"]] = np.array(train.loc[i][1:9]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb36853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # transforms for images\n",
    "        class args:\n",
    "            input_size = 224\n",
    "            imagenet_default_mean_and_std = True\n",
    "            color_jitter = 0.4\n",
    "            aa = \"rand-m9-mstd0.5-inc1\"\n",
    "            train_interpolation = \"bicubic\"\n",
    "            reprob = 0.25\n",
    "            remode = \"pixel\"\n",
    "            recount = 1\n",
    "            crop_pct = None\n",
    "\n",
    "\n",
    "        data_transforms = {\n",
    "            'train': build_transform(is_train = True, args=args),\n",
    "            'val': build_transform(is_train = False, args=args),\n",
    "        }\n",
    "        \n",
    "        data_train = '../Data/data70k/train/'\n",
    "        data_test = '../Data/data70k/test/'\n",
    "        data_val = '../Data/data70k/valid/'\n",
    "        \n",
    "        self.batch_size = 128\n",
    "        self.data_train = CustomDataSet(data_train, data_transforms['train'])\n",
    "        self.data_val = CustomDataSet(data_test, data_transforms['val'])\n",
    "                               \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data_train, \n",
    "#                           sampler=ImbalancedDatasetSampler(self.data_train),\n",
    "                          batch_size=self.batch_size, num_workers=8, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data_val, batch_size=self.batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbb3085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNext(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNext, self).__init__()\n",
    "        self.training_step_outputs = np.array([])\n",
    "        self.training_step_label = np.array([])\n",
    "        self.validation_step_outputs = np.array([])\n",
    "        self.validation_step_label = np.array([])\n",
    "        \n",
    "#         self.base_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "#         for param in self.base_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "        checkpoint = torch.load(\"./model_ckpt_1k_tiny/checkpoint-best.pth\")\n",
    "        \n",
    "        self.base_model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], num_classes=5)\n",
    "        self.base_model.load_state_dict(checkpoint[\"model\"])\n",
    "#         self.base_model = torch.nn.Sequential(*list(self.base.children())[:-1])\n",
    "#         self.base.eval()\n",
    "#         self.base_model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.convNext_clip = 776\n",
    "        \n",
    "        self.fc_au = nn.Sequential(\n",
    "            # nn.Dropout(p = 0.4),\n",
    "            nn.LayerNorm(self.convNext_clip),\n",
    "            nn.Linear(self.convNext_clip, self.convNext_clip),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p = 0.5),\n",
    "        )\n",
    "        \n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.convNext_clip),\n",
    "            nn.Linear(self.convNext_clip, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p = 0.3),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, num_classes),\n",
    "            nn.GELU(),\n",
    "#             nn.Linear(128, 1),\n",
    "#             nn.GELU(),\n",
    "        )\n",
    "        for param in self.reg_head.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "#     def MSEloss(self, outputs, labels):\n",
    "#         return torch.nn.MSELoss()(outputs, labels)\n",
    "    \n",
    "    def MAEloss(self, outputs, labels):\n",
    "        return nn.L1Loss()(outputs, labels)\n",
    "    \n",
    "    def MAE_3_4(self, outputs, labels):\n",
    "        pred_scores_3_4 = []\n",
    "        gt_scores_3_4 = []\n",
    "        \n",
    "        pred_scores_3 = []\n",
    "        gt_scores_3 = []\n",
    "        \n",
    "        pred_scores_4 = []\n",
    "        gt_scores_4 = []\n",
    "        test_mae_3_4 = 0\n",
    "        test_mse_3_4 = 0\n",
    "        test_mae_3 = 0\n",
    "        test_mse_3 = 0\n",
    "        test_mae_4 = 0\n",
    "        test_mse_4 = 0\n",
    "        \n",
    "        for idx, gt in enumerate(labels):\n",
    "            if(gt == 3 or gt == 4):\n",
    "                gt_scores_3_4.append(labels[idx])\n",
    "                pred_scores_3_4.append(outputs[idx])\n",
    "            if(gt == 3):\n",
    "                gt_scores_3.append(labels[idx])\n",
    "                pred_scores_3.append(outputs[idx])\n",
    "            if(gt == 4):\n",
    "                gt_scores_4.append(labels[idx])\n",
    "                pred_scores_4.append(outputs[idx])\n",
    "        \n",
    "        if(len(pred_scores_3_4) != 0):\n",
    "            test_mae_3_4 = mean_absolute_error(pred_scores_3_4, gt_scores_3_4)\n",
    "            test_mse_3_4 = mean_squared_error(pred_scores_3_4, gt_scores_3_4)\n",
    "        \n",
    "        if(len(pred_scores_3) != 0):\n",
    "            test_mae_3 = mean_absolute_error(pred_scores_3, gt_scores_3)\n",
    "            test_mse_3 = mean_squared_error(pred_scores_3, gt_scores_3)\n",
    "        \n",
    "        if(len(pred_scores_4) != 0):\n",
    "            test_mae_4 = mean_absolute_error(pred_scores_4, gt_scores_4)\n",
    "            test_mse_4 = mean_squared_error(pred_scores_4, gt_scores_4)\n",
    "        \n",
    "        return test_mae_3_4, test_mse_3_4, test_mae_3, test_mse_3, test_mae_4, test_mse_4\n",
    "\n",
    "    \n",
    "    def forward(self, x, x_clip):        \n",
    "        x1 = self.base_model(x)\n",
    "#         print(\"x\", x.size())\n",
    "\n",
    "        c = torch.cat((x_clip.squeeze(dim=1), x1), dim=1)\n",
    "#         print(c.size())\n",
    "\n",
    "#         print(x.size())\n",
    "        x = self.fc_au(c) + c\n",
    "#         print(x.size())\n",
    "        x = self.reg_head(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x    \n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, x_clip, y = train_batch\n",
    "        x_clip = x_clip.float()\n",
    "#         x = x.float()\n",
    "#         y = y.float()\n",
    "    \n",
    "    \n",
    "        logits = self.forward(x, x_clip)\n",
    "#         outputs = logits\n",
    "        outputs = torch.argmax(logits, -1)\n",
    "#         loss = self.MAEloss(logits, y)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        \n",
    "        \n",
    "        self.training_step_outputs = np.append(self.training_step_outputs, \n",
    "                                               outputs.detach().cpu().numpy()).flatten()\n",
    "        self.training_step_label = np.append(self.training_step_label, \n",
    "                                               y.detach().cpu().numpy()).flatten()\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, x_clip, y = val_batch\n",
    "        x_clip = x_clip.float()\n",
    "#         x = x.float()\n",
    "#         y = y.float()\n",
    "        \n",
    "        logits = self.forward(x, x_clip)\n",
    "#         outputs = logits\n",
    "        outputs = torch.argmax(logits, -1)\n",
    "#         loss = self.MAEloss(logits, y)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        \n",
    "        self.validation_step_outputs = np.append(self.validation_step_outputs, \n",
    "                                               outputs.detach().cpu().numpy()).flatten()\n",
    "        self.validation_step_label = np.append(self.validation_step_label, \n",
    "                                               y.detach().cpu().numpy()).flatten()\n",
    "    \n",
    "        \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        if(self.training_step_outputs.shape[0] != 0):\n",
    "            mse_loss = mean_squared_error(self.training_step_outputs, self.training_step_label)\n",
    "            mae_loss = mean_absolute_error(self.training_step_outputs, self.training_step_label)\n",
    "\n",
    "            print('training loss mse', mse_loss)\n",
    "            print('training loss mae', mae_loss)\n",
    "            self.training_step_outputs = np.array([])  # free memory\n",
    "            self.training_step_label = np.array([])  # free memory\n",
    "        \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        if(self.validation_step_outputs.shape[0] != 0):\n",
    "            mse_loss = mean_squared_error(self.validation_step_outputs, self.validation_step_label)\n",
    "            mae_loss = mean_absolute_error(self.validation_step_outputs, self.validation_step_label)\n",
    "            \n",
    "            test_mae_3_4, test_mse_3_4, test_mae_3, \\\n",
    "            test_mse_3, test_mae_4, test_mse_4 = self.MAE_3_4(self.validation_step_outputs, \n",
    "                                                         self.validation_step_label)\n",
    "            score = [test_mae_3_4, test_mse_3_4, test_mae_3, test_mse_3, test_mae_4, test_mse_4]\n",
    "\n",
    "            label = [\"test_mae_3_4\", \n",
    "                     \"test_mse_3_4\", \n",
    "                     \"test_mae_3\", \n",
    "                     \"test_mse_3\", \n",
    "                     \"test_mae_4\", \n",
    "                     \"test_mse_4\"]\n",
    "\n",
    "            for a,b in zip(label, score):\n",
    "                print(a, b)\n",
    "\n",
    "            print('validation loss mse', mse_loss)\n",
    "            print('validation loss mae', mae_loss)\n",
    "            \n",
    "            df = pd.DataFrame(self.validation_step_outputs, columns=['float'])\n",
    "            df.to_csv(\"./validation.csv\", index=False)\n",
    "            \n",
    "            self.validation_step_outputs = np.array([])  # free memory\n",
    "            self.validation_step_label = np.array([])  # free memory\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-2)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb8dab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7977, 20927, 11643, 1418, 33]\n",
      "[2660, 6976, 3882, 473, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | base_model | ConvNeXt   | 27.8 M\n",
      "1 | fc_au      | Sequential | 604 K \n",
      "2 | reg_head   | Sequential | 234 K \n",
      "------------------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.650   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 0\n",
      "test_mse_3_4 0\n",
      "test_mae_3 0\n",
      "test_mse_3 0\n",
      "test_mae_4 0\n",
      "test_mse_4 0\n",
      "validation loss mse 0.007547393281927215\n",
      "validation loss mae 0.06994104675322887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc6c4828b1d46778dfd918fae422da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 1.986750062339562\n",
      "test_mse_3_4 3.969386542613895\n",
      "test_mae_3 1.964022795443303\n",
      "test_mse_3 3.857385541023857\n",
      "test_mae_4 2.964022538878701\n",
      "test_mse_4 8.785429610985519\n",
      "validation loss mse 0.5993970000021884\n",
      "validation loss mae 0.5506789675271098\n",
      "training loss mse 0.6454377996735388\n",
      "training loss mae 0.5724767986400167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 1.9755366602219826\n",
      "test_mse_3_4 3.924955830347901\n",
      "test_mae_3 1.952809392271788\n",
      "test_mse_3 3.813464522547124\n",
      "test_mae_4 2.9528091820803555\n",
      "test_mse_4 8.71908206578131\n",
      "validation loss mse 0.5968042562795153\n",
      "validation loss mae 0.5548993377936873\n",
      "training loss mse 0.6073928597837984\n",
      "training loss mae 0.5455547407318441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0127400492341065\n",
      "test_mse_3_4 4.073333248081294\n",
      "test_mae_3 1.9900127772801017\n",
      "test_mse_3 3.9601508537381784\n",
      "test_mae_4 2.9900127432563086\n",
      "test_mse_4 8.94017620483528\n",
      "validation loss mse 0.6063729477816016\n",
      "validation loss mae 0.5408966439125487\n",
      "training loss mse 0.6080986445438029\n",
      "training loss mae 0.5414372305270234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0211305344892927\n",
      "test_mse_3_4 4.107179381011557\n",
      "test_mae_3 1.9984032618823062\n",
      "test_mse_3 3.9936155971018747\n",
      "test_mae_4 2.9984032565897163\n",
      "test_mse_4 8.990422089127863\n",
      "validation loss mse 0.6089135342535661\n",
      "validation loss mae 0.5377385911473613\n",
      "training loss mse 0.6086846992207606\n",
      "training loss mae 0.5394180403762706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0251623608849267\n",
      "test_mse_3_4 4.123493331746662\n",
      "test_mae_3 2.002435088157654\n",
      "test_mse_3 4.009746282284951\n",
      "test_mae_4 3.002435088157654\n",
      "test_mse_4 9.014616458600258\n",
      "validation loss mse 0.6101844225889157\n",
      "validation loss mae 0.538647438607681\n",
      "training loss mse 0.608901952441439\n",
      "training loss mae 0.5383868249585342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.021141669966958\n",
      "test_mse_3_4 4.107224393878476\n",
      "test_mae_3 1.998414397239685\n",
      "test_mse_3 3.9936601030948538\n",
      "test_mae_4 2.998414397239685\n",
      "test_mse_4 8.990488897574224\n",
      "validation loss mse 0.6089169675462759\n",
      "validation loss mae 0.5377343327058177\n",
      "training loss mse 0.6090654877346343\n",
      "training loss mae 0.5379898447148306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0206400372765283\n",
      "test_mse_3_4 4.105196904046542\n",
      "test_mae_3 1.9979127645492554\n",
      "test_mse_3 3.9916554147488483\n",
      "test_mae_4 2.9979127645492554\n",
      "test_mse_4 8.987480943847359\n",
      "validation loss mse 0.6087611048410586\n",
      "validation loss mae 0.5379231346111573\n",
      "training loss mse 0.6089959995774309\n",
      "training loss mae 0.5381919276181264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0206240632317285\n",
      "test_mse_3_4 4.105132348712752\n",
      "test_mae_3 1.9978967905044556\n",
      "test_mse_3 3.9915915855080044\n",
      "test_mae_4 2.9978967905044556\n",
      "test_mse_4 8.987385166516916\n",
      "validation loss mse 0.608756149800753\n",
      "validation loss mae 0.5379291468391315\n",
      "training loss mse 0.6090298080184803\n",
      "training loss mae 0.5379139073088406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.019583962180398\n",
      "test_mse_3_4 4.1009301240979275\n",
      "test_mae_3 1.996856689453125\n",
      "test_mse_3 3.987436638213694\n",
      "test_mae_4 2.996856689453125\n",
      "test_mse_4 8.981150017119944\n",
      "validation loss mse 0.6084346159370215\n",
      "validation loss mae 0.5383206146680497\n",
      "training loss mse 0.6090625443060319\n",
      "training loss mae 0.5378076734216538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0208095528862695\n",
      "test_mse_3_4 4.105881992838056\n",
      "test_mae_3 1.9980822801589966\n",
      "test_mse_3 3.9923327982853745\n",
      "test_mae_4 2.9980822801589966\n",
      "test_mse_4 8.988497358603368\n",
      "validation loss mse 0.6088137188763493\n",
      "validation loss mae 0.5378593332068339\n",
      "training loss mse 0.6089778461479868\n",
      "training loss mae 0.5379267096721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0197897174141626\n",
      "test_mse_3_4 4.101761246373634\n",
      "test_mae_3 1.9970624446868896\n",
      "test_mse_3 3.988258407978776\n",
      "test_mae_4 2.9970624446868896\n",
      "test_mse_4 8.982383297352555\n",
      "validation loss mse 0.6084980508503927\n",
      "validation loss mae 0.538243173582352\n",
      "training loss mse 0.6090195524709873\n",
      "training loss mae 0.5379455453103779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0201965787193994\n",
      "test_mse_3_4 4.103404960471218\n",
      "test_mae_3 1.9974693059921265\n",
      "test_mse_3 3.9898836283806673\n",
      "test_mae_4 2.9974693059921265\n",
      "test_mse_4 8.98482224036492\n",
      "validation loss mse 0.6086237365773765\n",
      "validation loss mae 0.5380900412385011\n",
      "training loss mse 0.6090139554241634\n",
      "training loss mae 0.5379105963373849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.024895034053109\n",
      "test_mse_3_4 4.122410642734592\n",
      "test_mae_3 2.002167761325836\n",
      "test_mse_3 4.008675744492509\n",
      "test_mae_4 3.002167761325836\n",
      "test_mse_4 9.013011267144181\n",
      "validation loss mse 0.6100991489360826\n",
      "validation loss mae 0.5384816814619989\n",
      "training loss mse 0.6090088160027008\n",
      "training loss mae 0.5379075740208438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0211845853111963\n",
      "test_mse_3_4 4.107397871701244\n",
      "test_mae_3 1.9984573125839233\n",
      "test_mse_3 3.993831630220157\n",
      "test_mae_4 2.9984573125839233\n",
      "test_mse_4 8.990746255388004\n",
      "validation loss mse 0.6089303251775955\n",
      "validation loss mae 0.5377181804515586\n",
      "training loss mse 0.6090370064481071\n",
      "training loss mae 0.5378897007397058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.020923040129922\n",
      "test_mse_3_4 4.106340677929618\n",
      "test_mae_3 1.998195767402649\n",
      "test_mse_3 3.992786324865861\n",
      "test_mae_4 2.998195767402649\n",
      "test_mse_4 8.989177859671159\n",
      "validation loss mse 0.6088489750171895\n",
      "validation loss mae 0.5378166194677931\n",
      "training loss mse 0.6090297141894092\n",
      "training loss mae 0.5378688623737259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.020790360190652\n",
      "test_mse_3_4 4.105804423641117\n",
      "test_mae_3 1.998063087463379\n",
      "test_mse_3 3.99225610148369\n",
      "test_mae_4 2.998063087463379\n",
      "test_mse_4 8.988382276410448\n",
      "validation loss mse 0.6088077589872517\n",
      "validation loss mae 0.5378665568538775\n",
      "training loss mse 0.6090797407205901\n",
      "training loss mae 0.5378263631706164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.023805699565194\n",
      "test_mse_3_4 4.118000253394214\n",
      "test_mae_3 2.001078426837921\n",
      "test_mse_3 4.0043148703561275\n",
      "test_mae_4 3.001078426837921\n",
      "test_mse_4 9.00647172403197\n",
      "validation loss mse 0.6097531437886562\n",
      "validation loss mae 0.5378062349526376\n",
      "training loss mse 0.6090656948919702\n",
      "training loss mae 0.537932361209442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.024211964823983\n",
      "test_mse_3_4 4.119644822338221\n",
      "test_mae_3 2.00148469209671\n",
      "test_mse_3 4.005940972697461\n",
      "test_mae_4 3.00148469209671\n",
      "test_mse_4 9.008910356890881\n",
      "validation loss mse 0.60988190825361\n",
      "validation loss mae 0.5380581414643364\n",
      "training loss mse 0.6090589397711532\n",
      "training loss mae 0.5378546777838916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.020243070342324\n",
      "test_mse_3_4 4.103592807067833\n",
      "test_mae_3 1.9975157976150513\n",
      "test_mse_3 3.990069361721694\n",
      "test_mae_4 2.9975157976150513\n",
      "test_mse_4 8.985100956951797\n",
      "validation loss mse 0.6086381196332545\n",
      "validation loss mae 0.5380725429630538\n",
      "training loss mse 0.6090118039745465\n",
      "training loss mae 0.5378536814433903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/mbbank/miniconda3/envs/gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mae_3_4 2.0218575217507104\n",
      "test_mse_3_4 4.110118582061577\n",
      "test_mae_3 1.9991302490234375\n",
      "test_mse_3 3.9965217525605112\n",
      "test_mae_4 2.9991302490234375\n",
      "test_mse_4 8.994782250607386\n",
      "validation loss mse 0.609140261980343\n",
      "validation loss mae 0.5374649041313015\n",
      "training loss mse 0.6090244031356338\n",
      "training loss mae 0.5379098813300122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "save_dir = './'\n",
    "data_module = DataModule()\n",
    "model = ConvNext(num_classes=5)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=save_dir, save_weights_only=True)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=[2], max_epochs=20)\n",
    "# trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=10)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9065f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed7453f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac95da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
